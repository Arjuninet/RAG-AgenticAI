{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import uvicorn\n",
    "import pickle\n",
    "from fastapi import FastAPI,Request\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "from openai import AzureOpenAI\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter,  CharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import JSONLoader, DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader,  UnstructuredPowerPointLoader, Docx2txtLoader, PyPDFLoader,  UnstructuredWordDocumentLoader, TextLoader, UnstructuredFileLoader\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,\n",
    "    SearchField,  \n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration\n",
    ") \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Defining a class for Tariff code prediction\n",
    "class Data(BaseModel):\n",
    "\n",
    "    EmailText: str\n",
    "\n",
    "#Initiating the FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Email Category Classification\",\n",
    "    description=\"A simple API that is used to classify the category of Email \",\n",
    "    version=\"0.1\",\n",
    "    debug=True\n",
    ")\n",
    "#Declaring the variables and environment\n",
    "open_ai_type = \"azure\"\n",
    "open_ai_api_version = \"2023-05-15\"\n",
    "open_ai_api_key = \"open_ai_api_key\"\n",
    "open_ai_api_key = open_ai_api_key \n",
    "open_ai_endpoint = \"open_ai_endpoint\" # base\n",
    "embeddings_model = \"janus-text-embedding-ada-002\"\n",
    "gpt_deployment_4 = \"janus-gpt-4\"\n",
    "gpt_deployment_35 = \"janus-gpt-35-turbo\"\n",
    "gpt_deployment_35_16=\"janus-gpt-35-turbo-16k\"\n",
    "embeddings_model = \"janus-text-embedding-ada-002\"\n",
    "vector_store_endpoint = \"vector_store_endpoint\"\n",
    "vector_store_key = \"vector_store_key\"\n",
    "vector_index_name = \"vector_index_name\"\n",
    "\n",
    "#Initiating the LLM, Embedding and Azure AI Search\n",
    "deployment_name = gpt_deployment_35\n",
    "llm_model = AzureChatOpenAI(deployment_name=deployment_name, openai_api_key =open_ai_api_key, azure_endpoint = open_ai_endpoint, openai_api_version = open_ai_api_version, temperature=0) \n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embeddings_model,\n",
    "    openai_api_version=open_ai_api_version,\n",
    "    azure_endpoint = open_ai_endpoint,\n",
    "    openai_api_key = open_ai_api_key\n",
    ")\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_endpoint ,\n",
    "    azure_search_key=vector_store_key,\n",
    "    index_name=vector_index_name,\n",
    "    embedding_function=embeddings.embed_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75568834",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(r\"your_path\", encoding=\"utf-8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a059faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75579064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)   \n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens, encoding\n",
    "\n",
    "print(num_tokens_from_string(documents[0].page_content, \"cl100k_base\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = texts[0:100] \n",
    "document2 = texts[100:200]\n",
    "document3 = texts[200:300]\n",
    "document4 = texts[300:400]\n",
    "document5 = texts[400:500] \n",
    "document6 = texts[500:600]\n",
    "document7 = texts[600:700]\n",
    "document8 = texts[700:800]\n",
    "document9 = texts[800:900]\n",
    "document10 = texts[900:1000]\n",
    "document11 = texts[1000:1100] \n",
    "document12 = texts[1100:1200]\n",
    "document13 = texts[1200:1300]\n",
    "document14 = texts[1300:1400]\n",
    "document15 = texts[1400:1500] \n",
    "document16 = texts[1500:1600]\n",
    "document17 = texts[1600:1700]\n",
    "document18 = texts[1700:1800]\n",
    "document19 = texts[1800:1900]\n",
    "document20 = texts[1900:2000]\n",
    "document21 = texts[2000:2100] \n",
    "document22 = texts[2100:2200]\n",
    "document23 = texts[2200:2300]\n",
    "document24 = texts[2300:2400]\n",
    "document25 = texts[2400:2500] \n",
    "document26 = texts[2500:2600]\n",
    "document27 = texts[2600:2700]\n",
    "document28 = texts[2700:2800]\n",
    "document29 = texts[2800:2900]\n",
    "document30 = texts[2900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee53e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document1)+len(document2)+len(document3)+len(document4)+len(document5)+len(document6)+len(document7)+len(document8)+len(document9)+len(document10)+len(document11)+len(document12)+len(document13)+len(document14)+len(document15)+len(document16)+len(document17)+len(document18)+len(document19)+len(document20)+len(document21)+len(document22)+len(document23)+len(document24)+len(document25)+len(document26)+len(document27)+len(document28)+len(document29)+len(document30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=texts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
