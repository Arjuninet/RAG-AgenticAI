{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def greet(name, intensity):\n",
    "  return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "demo = gr.Interface(\n",
    "  fn=greet,\n",
    "  inputs=[\"text\", \"slider\"],\n",
    "  outputs=[\"text\"],\n",
    ")\n",
    "demo.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f8127",
   "metadata": {},
   "source": [
    "## Image captioning using pretrained Hugging face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18063d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "# from PIL import Image\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# def generate_caption(image):\n",
    "#     # Now directly using the PIL Image object\n",
    "#     inputs = processor(images=image, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(**inputs)\n",
    "#     caption = processor.decode(outputs[0], skip_special_tokens=False)\n",
    "#     return caption\n",
    "\n",
    "# def caption_image(image):\n",
    "#     \"\"\"\n",
    "#     Takes a PIL Image input and returns a caption.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         caption = generate_caption(image)\n",
    "#         return caption\n",
    "#     except Exception as e:\n",
    "#         return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=caption_image,\n",
    "#     inputs=gr.Image(type=\"pil\"),\n",
    "#     outputs=\"text\",\n",
    "#     title=\"Image Captioning with BLIP\",\n",
    "#     description=\"Upload an image to generate a caption.\"\n",
    "# )\n",
    "\n",
    "# iface.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5330236",
   "metadata": {},
   "source": [
    "## Image Classification in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ec8dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/codespace/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6686c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3750f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download human-readable lables for Imagenet\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "lables = response.text.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp):\n",
    "    inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
    "    with torch.no_grad:\n",
    "        prediction = torch.nn.functional.softmax(model(inp)[0],dim=0)\n",
    "        confidence = {lables[i]:float(prediction[i] for i in range(1000))}\n",
    "    return confidence\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
