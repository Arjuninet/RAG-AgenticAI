{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def greet(name, intensity):\n",
    "  return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "demo = gr.Interface(\n",
    "  fn=greet,\n",
    "  inputs=[\"text\", \"slider\"],\n",
    "  outputs=[\"text\"],\n",
    ")\n",
    "demo.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f8127",
   "metadata": {},
   "source": [
    "## Image captioning using pretrained Hugging face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18063d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "# from PIL import Image\n",
    "\n",
    "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# def generate_caption(image):\n",
    "#     # Now directly using the PIL Image object\n",
    "#     inputs = processor(images=image, return_tensors=\"pt\")\n",
    "#     outputs = model.generate(**inputs)\n",
    "#     caption = processor.decode(outputs[0], skip_special_tokens=False)\n",
    "#     return caption\n",
    "\n",
    "# def caption_image(image):\n",
    "#     \"\"\"\n",
    "#     Takes a PIL Image input and returns a caption.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         caption = generate_caption(image)\n",
    "#         return caption\n",
    "#     except Exception as e:\n",
    "#         return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=caption_image,\n",
    "#     inputs=gr.Image(type=\"pil\"),\n",
    "#     outputs=\"text\",\n",
    "#     title=\"Image Captioning with BLIP\",\n",
    "#     description=\"Upload an image to generate a caption.\"\n",
    "# )\n",
    "\n",
    "# iface.launch(server_name=\"127.0.0.1\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5330236",
   "metadata": {},
   "source": [
    "## Image Classification in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ec8dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/codespace/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6686c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3750f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download human-readable lables for Imagenet\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "lables = response.text.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9294f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp):\n",
    "    inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.nn.functional.softmax(model(inp)[0],dim=0)\n",
    "        confidence = {lables[i]:float(prediction[i]) for i in range(1000)}\n",
    "    return confidence\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb7f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Interface(\n",
    "    fn = predict,\n",
    "    inputs = gr.Image(type='pil'),\n",
    "    outputs = gr.Label(num_top_classes=3),\n",
    "    examples = [\"/content/lion.jpg\", \"/content/cheetah.jpg\"]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa227033",
   "metadata": {},
   "source": [
    "## Simple LLM using Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cee980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/RAG-AgenticAI/venv/lib/python3.12/site-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://ef2317c20eae86a9f2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ef2317c20eae86a9f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "# from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "# from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "# from ibm_watsonx_ai import Credentials\n",
    "# from langchain_ibm import WatsonxLLM\n",
    "\n",
    "import gradio as gr\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Specify the model and project settings \n",
    "# (make sure the model you wish to use is commented out, and other models are commented)\n",
    "#model_id = 'mistralai/mixtral-8x7b-instruct-v01' # Specify the Mixtral 8x7B model\n",
    "model_id = 'llama-3.1-8b-instant' #'ibm/granite-3-3-8b-instruct' # Specify IBM's Granite 3.3 8B model\n",
    "gpt_model_id ='openai/gpt-oss-120b'\n",
    "# Set the necessary parameters\n",
    "# parameters = {\n",
    "#     GenParams.MAX_NEW_TOKENS: 256,  # Specify the max tokens you want to generate\n",
    "#     GenParams.TEMPERATURE: 0.3, # This randomness or creativity of the model's responses\n",
    "# }\n",
    "\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "# Wrap up the model into WatsonxLLM inference\n",
    "llm = ChatGroq(model=model_id, temperature=0.3)\n",
    "# watsonx_llm = WatsonxLLM(\n",
    "#     model_id=model_id,\n",
    "#     url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "#     project_id=project_id,\n",
    "#     params=parameters,\n",
    "# )\n",
    "\n",
    "# Get the query from the user input\n",
    "# query = input(\"Please enter your query: \")\n",
    "\n",
    "# # Print the generated response\n",
    "# print(watsonx_llm.invoke(query))\n",
    "\n",
    "def generate_response(prompt_txt):\n",
    "    generated_response = llm.invoke(prompt_txt)\n",
    "    return generated_response.content\n",
    "\n",
    "#Create Gradio interface\n",
    "chat_completion = gr.Interface(\n",
    "    fn = generate_response,\n",
    "    allow_flagging=\"never\",\n",
    "    inputs=gr.Textbox(label=\"Input\",lines=2,placeholder=\"Type your question here...\"),\n",
    "    outputs = gr.Textbox(label=\"Output\"),\n",
    "    title = \"Watsonx ai chatbot\",\n",
    "    description=\"Ask any question and the chatbot will try to answer\"\n",
    ")\n",
    "\n",
    "chat_completion.launch(server_name=\"127.0.0.1\", server_port=7860, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "028c40c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning (ML) is a subset of AI that enables systems to learn from data without explicit programming rules.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is ML in 20 words?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d71489ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model_id = 'openai/gpt-oss-120b'\n",
    "gpt_llm = ChatGroq(model=gpt_model_id, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58ce6d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning enables computers to learn patterns from data, improving performance on tasks without explicit programming and adapt over time.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_llm.invoke(\"what is ML in 20 words?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23adb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
